{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/redwoodresearch/Easy-Transformer.git\n",
        "%pip install einops datasets transformers fancy_einsum"
      ],
      "metadata": {
        "id": "8nA6bc6v3eUx"
      },
      "id": "8nA6bc6v3eUx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a76d69a1-ce05-470b-b9ea-84dd9824d9f5",
      "metadata": {
        "id": "a76d69a1-ce05-470b-b9ea-84dd9824d9f5"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch as t\n",
        "from easy_transformer.EasyTransformer import (\n",
        "    EasyTransformer,\n",
        ")\n",
        "from time import ctime\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "from easy_transformer.experiments import (\n",
        "    ExperimentMetric,\n",
        "    AblationConfig,\n",
        "    EasyAblation,\n",
        "    EasyPatching,\n",
        "    PatchingConfig,\n",
        ")\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import plotly.graph_objects as go\n",
        "import random\n",
        "import einops\n",
        "from IPython import get_ipython\n",
        "from copy import deepcopy\n",
        "from easy_transformer.ioi_dataset import (\n",
        "    IOIDataset,\n",
        ")\n",
        "from easy_transformer.ioi_utils import (\n",
        "    path_patching,\n",
        "    max_2d,\n",
        "    CLASS_COLORS,\n",
        "    show_pp,\n",
        "    show_attention_patterns,\n",
        "    scatter_attention_and_contribution,\n",
        ")\n",
        "from random import randint as ri\n",
        "from easy_transformer.ioi_circuit_extraction import (\n",
        "    do_circuit_extraction,\n",
        "    get_heads_circuit,\n",
        "    CIRCUIT,\n",
        ")\n",
        "from easy_transformer.ioi_utils import logit_diff, probs\n",
        "from easy_transformer.ioi_utils import get_top_tokens_and_probs as g\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "625b3a5b-b9b8-43c9-8d77-90eac50eab6c",
      "metadata": {
        "id": "625b3a5b-b9b8-43c9-8d77-90eac50eab6c",
        "outputId": "dcb0ae70-e472-4375-ccdc-48709376eedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving model to device:  cpu\n",
            "Finished loading pretrained model gpt2 into EasyTransformer!\n"
          ]
        }
      ],
      "source": [
        "model = EasyTransformer.from_pretrained(\"gpt2\")\n",
        "model.set_use_attn_result(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Task icl functions"
      ],
      "metadata": {
        "id": "U8Qt03qd48Uc"
      },
      "id": "U8Qt03qd48Uc"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "3882bde8-10f9-4f93-8615-25497560aee7",
      "metadata": {
        "id": "3882bde8-10f9-4f93-8615-25497560aee7"
      },
      "outputs": [],
      "source": [
        "class icl_dataset:\n",
        "    def __init__(self, input, labels, N, max_len, device):\n",
        "        self.input = input.to(device)\n",
        "        self.labels = labels\n",
        "        self.N = N\n",
        "        self.max_len = max_len\n",
        "        self.device = device\n",
        "\n",
        "def generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0):\n",
        "    prompts = []\n",
        "    correct_answers = []\n",
        "    # Initialize x and y for the sequence\n",
        "    x, y = x_initial + offset, y_initial + offset\n",
        "\n",
        "\n",
        "    for i in range(n):\n",
        "        prompt = ''\n",
        "        for j in range(icl_length):\n",
        "            if j < icl_length - 1:\n",
        "                prompt += f\"Input: {j}, Output: {j * x + y}\\n\"\n",
        "            else:\n",
        "                prompt += f\"Input: {j}, Output:\"\n",
        "                correct_answers.append(j * x + y)  # Record the correct answer for the last input\n",
        "        prompts.append(prompt)\n",
        "        # Update x and y after generating each full prompt set\n",
        "        if i % 2 == 0:\n",
        "            x += 1\n",
        "        else:\n",
        "            y += 1\n",
        "\n",
        "\n",
        "    # Convert prompts into tokens\n",
        "    data_tokens = model.to_tokens(prompts).to(device)\n",
        "    correct_answers_tensor = torch.tensor(correct_answers).to(torch.double).unsqueeze(-1).to(device)\n",
        "    return icl_dataset(input=data_tokens, labels=correct_answers_tensor, N=n, max_len = data_tokens.shape[1], device = device)\n",
        "\n",
        "def validation_metric(model, dataset, device='cpu', return_one_element = False):\n",
        "        # dataset: {input: data, labels: correct, }\n",
        "        logits = model(dataset.input, return_type=\"logits\")\n",
        "\n",
        "\n",
        "        # Select the logits for the last token in each sequence\n",
        "        # model_output shape: [batch_size, seq_length, vocab_size] => [10, 103, 50257]\n",
        "        # We select [:, -1, :] to get the last token logits for each example in the batch\n",
        "        last_token_logits = logits[:, -1, :]  # Shape: [10, 50257]\n",
        "\n",
        "        # Now, find the indices of the 10 highest logits for the last token across the batch\n",
        "        # We use torch.topk to get the top 10 logits' indices for each example\n",
        "        topk_values, topk_indices = torch.topk(last_token_logits, 1, dim=1)\n",
        "\n",
        "        predictions = model.to_str_tokens(topk_indices)\n",
        "        predictions = torch.tensor([int(pred) for pred in predictions]).to(torch.double).unsqueeze(-1).to(device)\n",
        "\n",
        "        # Calculate MSE\n",
        "        mse = F.mse_loss(predictions, dataset.labels, reduction='mean' if not return_one_element else 'sum')\n",
        "        return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "feb1ea28-4ec8-41bb-98c7-8af7dd74e1f2",
      "metadata": {
        "id": "feb1ea28-4ec8-41bb-98c7-8af7dd74e1f2",
        "outputId": "b70de7e5-44c0-4f17-8574-05162fb2c129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(61.4000, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "def test_validation_metric(device, model, x_initial, y_initial, icl_length, n):\n",
        "        dataset = generate_data(model, device, x_initial, y_initial, icl_length, n, 10)\n",
        "        mse = validation_metric(model, dataset)\n",
        "\n",
        "        return mse\n",
        "        print('This is the MSE: ', mse)\n",
        "test_validation_metric('cpu', model, 2, 1, 12, 10)  # 3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74053c60-fa13-4e92-bc69-c0cd4e1af319",
      "metadata": {
        "id": "74053c60-fa13-4e92-bc69-c0cd4e1af319"
      },
      "source": [
        "# Ablating functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "d9227d6d-533a-4b98-b730-bf2a74b8b1e2",
      "metadata": {
        "id": "d9227d6d-533a-4b98-b730-bf2a74b8b1e2"
      },
      "outputs": [],
      "source": [
        "def list_diff(l1, l2):\n",
        "    #print(l1, l2)\n",
        "    l2_ = [int(x) for x in l2]\n",
        "    return list(set(l1).difference(set(l2_)))\n",
        "def turn_keep_into_rmv(to_keep, max_len):\n",
        "    to_rmv = {}\n",
        "    for t in to_keep.keys():\n",
        "        to_rmv[t] = []\n",
        "        for idxs in to_keep[t]:\n",
        "            #to_rmv[t].append(list_diff(list(range(max_len)), idxs))\n",
        "            if idxs == []:\n",
        "              to_rmv[t].append(list(range(max_len)))\n",
        "            else:\n",
        "              to_rmv[t].append([])\n",
        "    return to_rmv\n",
        "def process_heads_and_mlps(\n",
        "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "    heads_to_keep=None,  # as above for heads\n",
        "    mlps_to_keep=None,  # as above for mlps\n",
        "    ioi_dataset=None,\n",
        "    model=None,\n",
        "):\n",
        "    assert (heads_to_remove is None) != (heads_to_keep is None)\n",
        "    assert (mlps_to_keep is None) != (mlps_to_remove is None)\n",
        "\n",
        "    n_layers = model.cfg.n_layers\n",
        "    n_heads = model.cfg.n_heads\n",
        "\n",
        "    dataset_length = ioi_dataset.max_len\n",
        "\n",
        "    #commented out since I only want to remove attention\n",
        "    if mlps_to_remove is not None:\n",
        "        mlps = mlps_to_remove.copy()\n",
        "    else:  # MARCO, if list of mlps to remove available just use, otherwise remove all not in 'to keep'. it do smart computation in mean cache\n",
        "        mlps = mlps_to_keep.copy()\n",
        "        for l in range(n_layers):\n",
        "            if l not in mlps_to_keep:\n",
        "                mlps[l] = [[] for _ in range(dataset_length)]\n",
        "        mlps = turn_keep_into_rmv(\n",
        "            mlps, ioi_dataset.max_len\n",
        "        )  # TODO check that this is still right for the max_len of maybe shortened datasets\n",
        "\n",
        "    # MARCO Same as MLP above\n",
        "    if heads_to_remove is not None:\n",
        "        heads = heads_to_remove.copy()\n",
        "    else:\n",
        "        heads = heads_to_keep.copy()\n",
        "\n",
        "        for l in range(n_layers):\n",
        "            for h in range(n_heads):\n",
        "                if (l, h) not in heads_to_keep:\n",
        "                    heads[(l, h)] = [[] for _ in range(dataset_length)]\n",
        "        heads = turn_keep_into_rmv(heads, ioi_dataset.max_len)\n",
        "    return heads, mlps\n",
        "    # print(mlps, heads)\n",
        "\n",
        "def get_circuit_replacement_hook(\n",
        "    heads_to_remove=None,\n",
        "    mlps_to_remove=None,\n",
        "    heads_to_keep=None,\n",
        "    mlps_to_keep=None,\n",
        "    heads_to_remove2=None,  # TODO @Alex ehat are these\n",
        "    mlps_to_remove2=None,\n",
        "    heads_to_keep2=None,\n",
        "    mlps_to_keep2=None,\n",
        "    ioi_dataset=None,\n",
        "    model=None,\n",
        "):\n",
        "    # MARCO function above, just get a list\n",
        "    heads, mlps = process_heads_and_mlps(\n",
        "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "        heads_to_keep=heads_to_keep,  # as above for heads\n",
        "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
        "        ioi_dataset=ioi_dataset,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "    if (heads_to_remove2 is not None) or (heads_to_keep2 is not None):\n",
        "        heads2, mlps2 = process_heads_and_mlps(\n",
        "            heads_to_remove=heads_to_remove2,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "            mlps_to_remove=mlps_to_remove2,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "            heads_to_keep=heads_to_keep2,  # as above for heads\n",
        "            mlps_to_keep=mlps_to_keep2,  # as above for mlps\n",
        "            ioi_dataset=ioi_dataset,\n",
        "            model=model,\n",
        "        )\n",
        "    else:\n",
        "        heads2, mlps2 = heads, mlps\n",
        "\n",
        "    dataset_length = ioi_dataset.N\n",
        "\n",
        "    def circuit_replmt_hook(z, act, hook):  # batch, seq, heads, head dim\n",
        "        layer = int(hook.name.split(\".\")[1])\n",
        "        if False or \"mlp\" in hook.name and layer in mlps:\n",
        "            for i in range(dataset_length):\n",
        "                z[i, mlps[layer][i], :] = act[\n",
        "                    i, mlps2[layer][i], :\n",
        "                ]  # ablate all the indices in mlps[layer][i]; mean may contain semantic ablation\n",
        "                # TODO can this i loop be vectorized?\n",
        "\n",
        "        if \"attn.hook_result\" in hook.name and (layer, hook.ctx[\"idx\"]) in heads:\n",
        "            for i in range(\n",
        "                dataset_length\n",
        "            ):  # we use the idx from contex to get the head\n",
        "                z[i, heads[(layer, hook.ctx[\"idx\"])][i], :] = act[\n",
        "                    i,\n",
        "                    heads2[(layer, hook.ctx[\"idx\"])][i],\n",
        "                    :,\n",
        "                ]\n",
        "\n",
        "        return z\n",
        "\n",
        "    return circuit_replmt_hook, heads, mlps\n",
        "\n",
        "def do_circuit_extraction(\n",
        "    heads_to_remove=None,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "    mlps_to_remove=None,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "    heads_to_keep=None,  # as above for heads\n",
        "    mlps_to_keep=None,  # as above for mlps\n",
        "    ioi_dataset=None,\n",
        "    mean_dataset=None,\n",
        "    model=None,\n",
        "    metric=None,\n",
        "    excluded=[],  # tuple of (layer, head) or (layer, None for MLPs)\n",
        "    return_hooks=False,\n",
        "    hooks_dict=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    ..._to_remove means the indices ablated away. Otherwise the indices not ablated away.\n",
        "\n",
        "    `exclude_heads` is a list of heads that actually we won't put any hooks on. Just keep them as is\n",
        "\n",
        "    if `mean_dataset` is None, just use the ioi_dataset for mean\n",
        "    \"\"\"\n",
        "\n",
        "    # check if we are either in keep XOR remove move from the args\n",
        "    ablation, heads, mlps = get_circuit_replacement_hook(\n",
        "        heads_to_remove=heads_to_remove,  # {(2,3) : List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "        mlps_to_remove=mlps_to_remove,  # {2: List[List[int]]: dimensions dataset_size * datapoint_length\n",
        "        heads_to_keep=heads_to_keep,  # as above for heads\n",
        "        mlps_to_keep=mlps_to_keep,  # as above for mlps\n",
        "        ioi_dataset=ioi_dataset,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "    metric = ExperimentMetric(\n",
        "        metric=metric, dataset=ioi_dataset.input, relative_metric=False\n",
        "    )  # TODO make dummy metric\n",
        "\n",
        "    if mean_dataset is None:\n",
        "        mean_dataset = ioi_dataset\n",
        "\n",
        "    config = AblationConfig(\n",
        "        abl_type=\"custom\",\n",
        "        abl_fn=ablation,\n",
        "        mean_dataset=mean_dataset.input.long(),\n",
        "        target_module=\"attn_head\",\n",
        "        head_circuit=\"result\",\n",
        "        cache_means=True,  # circuit extraction *has* to cache means. the get_mean reset the\n",
        "        verbose=True,\n",
        "    )\n",
        "    abl = EasyAblation(\n",
        "        model,\n",
        "        config,\n",
        "        metric,\n",
        "        semantic_indices=None,  # ioi_dataset.sem_tok_idx,\n",
        "    )\n",
        "    model.reset_hooks()\n",
        "\n",
        "    hooks = {}\n",
        "\n",
        "    heads_keys = list(heads.keys())\n",
        "    # sort in lexicographic order\n",
        "    heads_keys.sort(key=lambda x: (x[0], x[1]))\n",
        "\n",
        "    for (\n",
        "        layer,\n",
        "        head,\n",
        "    ) in heads_keys:  # a sketchy edit here didn't really improve things : (\n",
        "        if (layer, head) in excluded:\n",
        "            continue\n",
        "        assert (layer, head) not in hooks, ((layer, head), \"already in hooks\")\n",
        "        hooks[(layer, head)] = abl.get_hook(layer, head)\n",
        "        # model.add_hook(*abl.get_hook(layer, head))\n",
        "    for layer in mlps.keys():\n",
        "        hooks[(layer, None)] = abl.get_hook(layer, head=None, target_module=\"mlp\")\n",
        "        # model.add_hook(*abl.get_hook(layer, head=None, target_module=\"mlp\"))\n",
        "\n",
        "    if return_hooks:\n",
        "        if hooks_dict:\n",
        "            return hooks\n",
        "        else:\n",
        "            return list(hooks.values())\n",
        "\n",
        "    else:\n",
        "        for hook in hooks.values():\n",
        "            model.add_hook(*hook)\n",
        "        return model, abl\n",
        "\n",
        "def get_heads_circuit(ioi_dataset, excluded=[], mlp0=False, circuit=CIRCUIT):\n",
        "\n",
        "    for excluded_thing in excluded:\n",
        "        assert (\n",
        "            isinstance(excluded_thing, tuple) or excluded_thing in circuit.keys()\n",
        "        ), excluded_thing\n",
        "\n",
        "    heads_to_keep = {}\n",
        "\n",
        "\n",
        "    for circuit_class in circuit.keys():\n",
        "        if circuit_class in excluded:\n",
        "            continue\n",
        "        for head in circuit[circuit_class]:\n",
        "            if head in excluded:\n",
        "                continue\n",
        "            heads_to_keep[head] = list(range(ioi_dataset.input.shape[1]))\n",
        "\n",
        "    if mlp0:\n",
        "        raise ValueError(\"Arthur moved this to get_mlps_circuit\")\n",
        "    return heads_to_keep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check mse diff"
      ],
      "metadata": {
        "id": "JwppAzjs4loH"
      },
      "id": "JwppAzjs4loH"
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "819ca83d-f2ce-4b81-8c76-b0092681b12e",
      "metadata": {
        "id": "819ca83d-f2ce-4b81-8c76-b0092681b12e",
        "outputId": "2092abfb-11ff-44f5-d4b7-021daf30f17b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The circuit has mse 15570.5 over 10 examples. Full model mse was 3.2\n"
          ]
        }
      ],
      "source": [
        "circuit = {\n",
        "  'operating': [(8, 1), (6, 9), (5, 0)]\n",
        "}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "N = 10\n",
        "\n",
        "# we make the ABC dataset in order to knockout other model components\n",
        "# generate_data(model, device, x_initial, y_initial, icl_length, n, offset=0)\n",
        "base_dataset = generate_data(model=model, device=device, x_initial=2, y_initial=1, icl_length=12, n=N, offset=0)\n",
        "benchmark_mse = validation_metric(model, base_dataset)\n",
        "\n",
        "patch_dataset = generate_data(model=model, device=device, x_initial=2, y_initial=1, icl_length=12, n=N, offset=10)\n",
        "benchmark_patch_mse = validation_metric(model, patch_dataset)\n",
        "\n",
        "# we then add hooks to the model to knockout all the heads except the circuit\n",
        "model.reset_hooks()\n",
        "ablated_model, _ = do_circuit_extraction(\n",
        "    model=model,\n",
        "    heads_to_keep=get_heads_circuit(ioi_dataset=base_dataset, circuit=circuit),\n",
        "    mlps_to_remove={},\n",
        "    ioi_dataset=base_dataset,\n",
        "    mean_dataset=patch_dataset\n",
        ")\n",
        "\n",
        "circuit_mse = validation_metric(ablated_model, base_dataset)\n",
        "print(\n",
        "    f\"The circuit has mse {circuit_mse} over {N} examples. Full model mse was {benchmark_mse}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try model"
      ],
      "metadata": {
        "id": "3aHfCx904OFY"
      },
      "id": "3aHfCx904OFY"
    },
    {
      "cell_type": "code",
      "source": [
        "reference_text = \"\"\"Google decreased revenue by 50% this quarter. // Sell\n",
        "Apple increased revenue by 50% this quarter. // Buy\n",
        "OpenAI increased revenue by 50% this quarter. // Buy\n",
        "Tesla decreased revenue by 50% this quarter. // Sell\n",
        "Audi increased revenue by 50% this quarter. // Buy\n",
        "Microsoft decreased revenue by 50% this quarter. // Sell\n",
        "Ford decreased decreased revenue by 40% this quarter. //\"\"\"\n",
        "\n",
        "tokens = ablated_model.to_tokens(reference_text)\n",
        "print(tokens)\n",
        "print(tokens.shape)\n",
        "print(ablated_model.to_str_tokens(tokens))\n",
        "\n",
        "\n",
        "logits, cache = ablated_model.run_with_cache(tokens)\n",
        "list(zip(ablated_model.to_str_tokens(reference_text), ablated_model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
      ],
      "metadata": {
        "id": "0oPLsqJ397Ob"
      },
      "id": "0oPLsqJ397Ob",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_metric(model, base_dataset)"
      ],
      "metadata": {
        "id": "sEL3wYAqJuCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7392610d-6b7c-4e2c-ad17-5f197f564dc3"
      },
      "id": "sEL3wYAqJuCo",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.2000, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks()"
      ],
      "metadata": {
        "id": "gIDlBYe2Jx6P"
      },
      "id": "gIDlBYe2Jx6P",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_metric(model, patch_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMWmDl3kcc6D",
        "outputId": "a41354c5-3e5f-4a98-86b0-1a13bc82d901"
      },
      "id": "LMWmDl3kcc6D",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(61.4000, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_model.reset_hooks()"
      ],
      "metadata": {
        "id": "G7_cXFJycoa6"
      },
      "id": "G7_cXFJycoa6",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Try IOI"
      ],
      "metadata": {
        "id": "M9rNvNH50Rh5"
      },
      "id": "M9rNvNH50Rh5"
    },
    {
      "cell_type": "code",
      "source": [
        "RELEVANT_TOKENS = {}\n",
        "for head in CIRCUIT[\"name mover\"] + CIRCUIT[\"negative\"] + CIRCUIT[\"s2 inhibition\"]:\n",
        "    RELEVANT_TOKENS[head] = [\"end\"]\n",
        "\n",
        "for head in CIRCUIT[\"induction\"]:\n",
        "    RELEVANT_TOKENS[head] = [\"S2\"]\n",
        "\n",
        "for head in CIRCUIT[\"duplicate token\"]:\n",
        "    RELEVANT_TOKENS[head] = [\"S2\"]\n",
        "\n",
        "for head in CIRCUIT[\"previous token\"]:\n",
        "    RELEVANT_TOKENS[head] = [\"S+1\"]\n",
        "\n",
        "\n",
        "CIRCUIT = {\n",
        "    \"name mover\": [\n",
        "        (9, 9),  # by importance\n",
        "        (10, 0),\n",
        "        (9, 6),\n",
        "        (10, 10),\n",
        "        (10, 6),\n",
        "        (10, 2),\n",
        "        (10, 1),\n",
        "        (11, 2),\n",
        "        (9, 7),\n",
        "        (9, 0),\n",
        "        (11, 9),\n",
        "    ],\n",
        "    \"negative\": [(10, 7), (11, 10)],\n",
        "    \"s2 inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
        "    \"induction\": [(5, 5), (5, 8), (5, 9), (6, 9)],\n",
        "    \"duplicate token\": [\n",
        "        (0, 1),\n",
        "        (0, 10),\n",
        "        (3, 0),\n",
        "        # (7, 1),\n",
        "    ],  # unclear exactly what (7,1) does\n",
        "    \"previous token\": [\n",
        "        (2, 2),\n",
        "        # (2, 9),\n",
        "        (4, 11),\n",
        "        # (4, 3),\n",
        "        # (4, 7),\n",
        "        # (5, 6),\n",
        "        # (3, 3),\n",
        "        # (3, 7),\n",
        "        # (3, 6),\n",
        "    ],\n",
        "}\n",
        "from easy_transformer.ioi_dataset import IOIDataset\n",
        "\n",
        "# we make the ABC dataset in order to knockout other model components\n",
        "\n",
        "ioi_dataset = IOIDataset(\n",
        "    prompt_type=\"mixed\",\n",
        "    N=50,\n",
        "    tokenizer=model.tokenizer,\n",
        "    prepend_bos=False,\n",
        ")  # TODO make this a seeded dataset\n",
        "\n",
        "abc_dataset = (  # TODO seeded\n",
        "    ioi_dataset.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
        "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
        "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
        ")\n",
        "# we then add hooks to the model to knockout all the heads except the circuit\n",
        "model.reset_hooks()\n",
        "model, _ = do_circuit_extraction(\n",
        "    model=model,\n",
        "    heads_to_keep=get_heads_circuit(ioi_dataset=ioi_dataset, circuit=CIRCUIT),\n",
        "    mlps_to_remove={},\n",
        "    ioi_dataset=ioi_dataset,\n",
        "    mean_dataset=abc_dataset,\n",
        ")\n",
        "\n",
        "circuit_logit_diff = logit_diff(model, ioi_dataset)\n",
        "print(\n",
        "    f\"The circuit gets average logit difference {circuit_logit_diff.item()} over {50} examples\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQtsquM-0VSs",
        "outputId": "befe6945-c748-4368-8f43-65c0283e1809"
      },
      "id": "UQtsquM-0VSs",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easy_transformer/ioi_dataset.py:769: UserWarning: Some groups have less than 5 prompts, they have lengths [4, 1, 2, 1, 3, 4, 3, 2, 4, 2, 2]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The circuit gets average logit difference 2.97226619720459 over 50 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logit_diff(model, ioi_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8kgPb--2bn6",
        "outputId": "dfb6f880-a07e-4470-9497-01720ea015e5"
      },
      "id": "P8kgPb--2bn6",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.9723)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_text = \"\"\"Google decreased revenue by 50% this quarter. // Sell\n",
        "Apple increased revenue by 50% this quarter. // Buy\n",
        "OpenAI increased revenue by 50% this quarter. // Buy\n",
        "Tesla decreased revenue by 50% this quarter. // Sell\n",
        "Audi increased revenue by 50% this quarter. // Buy\n",
        "Microsoft decreased revenue by 50% this quarter. // Sell\n",
        "Ford decreased decreased revenue by 40% this quarter. //\"\"\"\n",
        "\n",
        "tokens = model.to_tokens(reference_text)\n",
        "print(tokens)\n",
        "print(tokens.shape)\n",
        "print(model.to_str_tokens(tokens))\n",
        "\n",
        "\n",
        "logits, cache = model.run_with_cache(tokens)\n",
        "list(zip(model.to_str_tokens(reference_text), model.tokenizer.batch_decode(logits.argmax(dim=-1)[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atyfkS0z2gV3",
        "outputId": "079c90d3-9a0f-46c9-c6ff-7f83135f884a"
      },
      "id": "atyfkS0z2gV3",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[50256, 11708, 11832,  6426,   416,  2026,     4,   428,  3860,    13,\n",
            "          3373, 25688,   198, 16108,  3220,  6426,   416,  2026,     4,   428,\n",
            "          3860,    13,  3373, 11763,   198, 11505, 20185,  3220,  6426,   416,\n",
            "          2026,     4,   428,  3860,    13,  3373, 11763,   198, 41351, 11832,\n",
            "          6426,   416,  2026,     4,   428,  3860,    13,  3373, 25688,   198,\n",
            "         16353,    72,  3220,  6426,   416,  2026,     4,   428,  3860,    13,\n",
            "          3373, 11763,   198, 15905, 11832,  6426,   416,  2026,     4,   428,\n",
            "          3860,    13,  3373, 25688,   198, 37308, 11832, 11832,  6426,   416,\n",
            "          2319,     4,   428,  3860,    13,  3373]])\n",
            "torch.Size([1, 86])\n",
            "['<|endoftext|>', 'Google', ' decreased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Sell', '\\n', 'Apple', ' increased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Buy', '\\n', 'Open', 'AI', ' increased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Buy', '\\n', 'Tesla', ' decreased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Sell', '\\n', 'Aud', 'i', ' increased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Buy', '\\n', 'Microsoft', ' decreased', ' revenue', ' by', ' 50', '%', ' this', ' quarter', '.', ' //', ' Sell', '\\n', 'Ford', ' decreased', ' decreased', ' revenue', ' by', ' 40', '%', ' this', ' quarter', '.', ' //']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<|endoftext|>', '\\n'),\n",
              " ('Google', ' has'),\n",
              " (' decreased', ' its'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' $'),\n",
              " (' 50', '%'),\n",
              " ('%', ' in'),\n",
              " (' this', ' year'),\n",
              " (' quarter', ','),\n",
              " ('.', '\\n'),\n",
              " (' //', ' Google'),\n",
              " (' Sell', ' your'),\n",
              " ('\\n', '\\n'),\n",
              " ('Apple', \"'s\"),\n",
              " (' increased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 50', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Sell'),\n",
              " (' Buy', '\\n'),\n",
              " ('\\n', 'Google'),\n",
              " ('Open', ' source'),\n",
              " ('AI', ' increased'),\n",
              " (' increased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 50', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Sell'),\n",
              " (' Buy', '\\n'),\n",
              " ('\\n', 'Google'),\n",
              " ('Tesla', ' increased'),\n",
              " (' decreased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 50', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Buy'),\n",
              " (' Sell', '\\n'),\n",
              " ('\\n', '\\n'),\n",
              " ('Aud', 'ience'),\n",
              " ('i', ' increased'),\n",
              " (' increased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 50', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Sell'),\n",
              " (' Buy', '\\n'),\n",
              " ('\\n', 'Microsoft'),\n",
              " ('Microsoft', ' increased'),\n",
              " (' decreased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 50', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Sell'),\n",
              " (' Sell', '\\n'),\n",
              " ('\\n', '\\n'),\n",
              " ('Ford', ' increased'),\n",
              " (' decreased', ' revenue'),\n",
              " (' decreased', ' revenue'),\n",
              " (' revenue', ' by'),\n",
              " (' by', ' 50'),\n",
              " (' 40', '%'),\n",
              " ('%', ' this'),\n",
              " (' this', ' quarter'),\n",
              " (' quarter', '.'),\n",
              " ('.', ' //'),\n",
              " (' //', ' Sell')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}